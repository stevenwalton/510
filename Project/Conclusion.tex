\section{Conclusion}
While our Q-Learner did not consistently perform better than the brute forcing
algorithm on every tested game, there were some clear areas that the learner
beat the brute forcer. When the state space was substantially larger the 
approximating Q-Learner was able to learn faster by using lookaheads and 
exploring its environment more than the greedy algorithm could. From testing
we were able to conclude that dept and exploration had significant impacts on the
learning rate of the algorithm. While there was a lack of resources and 
optimization in the code, these experiments show great promise in Q-Learning
being able to solve extremely complex problems and warrants further studying.

