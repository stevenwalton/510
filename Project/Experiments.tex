\section{Experiments}
Since much of the project was spent on implementation there was little time
left for experimentation and testing. We were able to test a few different 
options and test multiple games. There were many more runs than are included
in here but not all were recorded as we were testing that different parameters
worked and were were focusing on creating good videos for the presentation.

It is key to note that the default values of hyper parameters are:
discount=0.8, frame skips=4, learning rate=0.8, depth=1, explore=0, 
random action=0.8. The discount is $\gamma$ in Equation~\ref{eq:Bell}, similarly
learning rate is $\alpha$ in the same equation. Frame skips are the number of
frames that the game skips when emulating, this helps reduce the number of 
states and can help learning if adjacent states are extremely similar. Depth
is the number of lookaheads that we perform (amount of recursion in Bellman's
Equation). Explore is the k value in the exploration function, Equation
\ref{eq:explore}.

Figure~\ref{fig:AGLearn} shows the results for Airstriker-Genesis, the 
default game for Retro. To perform this experiment a timer feature was
added to the program to denote the time when the new best reward was obtained.
These programs were run for the same amount of time.
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\textwidth]{AGLearners.png}
    \caption{Comparison of different hyper-parameters on Airstriker-Genesis}
    \label{fig:AGLearn}
\end{figure}
While this graph shows the brute forcing (greedy) algorithm greatly out
performing our Q-Learner this was not always observed in practice. With
limited time it is difficult to make accurate conclusions and properly tune
hyper-parameters. Experimentation found that performance on specific games
were extremely dependent upon the hyper-parameters. The Q-Learner has a tendency
to get trapped in local optima and unfortunately without good parameters 
the learner does not perform as well. Seeing how many of the parameters
do not extend along the x-axis it appears that these learners got trapped in 
local optima. Observing this we can see how important the exploration function
is to help the learner escape these.

\FloatBarrier
The next thing to look at is the harder game, Super Mario Bros (SMB). Figure
\ref{fig:SMBLearn} shows different hyper-parameters configured to play
SMB. All of these were again run for the same amount of time. I changed the 
graph slightly because everything got stuck in a local optima except for the 
parameter with $\gamma=0.1$ depth$=50$ and exploration$=1$ (depth$=100$ likely
would have updated again). These values would likely have continued updating 
given more time but given the constraints I repeated their final scores with 
a time of 5000 seconds so that it was easier to visually see what happened.
There was no manipulation of data as these were the values at 5000 seconds. 
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\textwidth]{SMBLearners.png}
    \caption{Comparison of different hyper-parameters on Super Mario Bros}
    \label{fig:SMBLearn}
\end{figure}
Here we can see that every Q-Learner performed much better than the brute forcer
algorithm. 

\FloatBarrier
We found that when playing a PSPACE-Hard game like DonkeyKong Country
that our Q-Learner more easily beat the brute forcing algorithm. This can
be seen in Figure~\ref{fig:DKCLearn}. DonkeyKong Country is specifically
a difficult game, as there are substantially more states and actions than 
Airstriker-Genesis. There are also two characters that can be played and 
swapped out, that each have different attributes. One of the sections that the
learner consistently got stuck on was when it was using the worse agent at
a particular section.
The learner that ran with a depth of 100 (depth of lookahead) was not run as 
long as the other brute and default. There were constraints of how many 
learners could be run at a single time on our system. Additionally
depth 100 explore 1 was run at a different time and showed more steady growth.
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\textwidth]{DKCLearners.png}
    \caption{Comparison of different hyper-parameters on DonkeyKong Country}
    \label{fig:DKCLearn}
\end{figure}

From these experiments we can make some conclusions about how Q-Learning works
in different games. It seems that when the game is sufficiently complex that
it performs much better than a greedy algorithm. It seems that we may have just
lucked out with the default values. Additionally I had run some examples over
the weekend but did not record the times associated with each step, so they 
are not included in this analysis. In those examples I had found that a high 
depth and exploration greatly helped the learners escape local optima. 
Similarly there were experiments run with frame skipping. It was found that
games like DKC and SMB performed better with a slightly higher frame skip, moreso
DKC than SMB. We hypothesise that this is because in these games there is more 
similarity between frames, or states. Specifically DKC performed better with 
this because it was not reaching enemies, which is where reaction time matters
more. It is also believed that given more resources and time that these
algorithms would actually be able to do significantly better. For comparison
similar experiments by OpenAI, DeepMind, and others use hundreds of GPUs, 
whereas currently these algorithms are single-threaded and run on CPUs. 
Those systems also ran for hundreds or thousands of compute hours, where
our maximal value here is just under 3 hours.
There is clearly a lot of optimization that can be done that would greatly 
help improve these programs.

